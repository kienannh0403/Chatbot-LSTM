{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSVCII1Mt4-h"
      },
      "source": [
        "from glob import glob\n",
        "from  sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCNiZWicuFqJ",
        "outputId": "90c4b5e5-cc09-4fdb-b407-892e3bd2bb3f"
      },
      "source": [
        "files = glob(\"data/*.txt\")\n",
        "questions_cleaned = []\n",
        "answers_cleaned = []\n",
        "\n",
        "for file in files:\n",
        "  lines = open(file, 'r').readlines()\n",
        "  for line in lines:\n",
        "    split_line = line.split(\"__eou__\", 2)\n",
        "    questions_cleaned.append(re.sub(r'\\W+',\" \", split_line[0]).strip().lower())\n",
        "\n",
        "    answers_cleaned.append(re.sub(r'\\W+',\" \", split_line[1]).strip().lower())\n",
        "\n",
        "print(f\"{questions_cleaned[:3]}\\n{answers_cleaned[:3]}\")\n",
        "print(f\"Len questions_cleaned: {len(questions_cleaned)}\")\n",
        "print(f\"Len answers_cleaned: {len(answers_cleaned)}\")\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['xin chào mình có thể giúp gì cho bạn', 'bạn muốn một chiếc điện thoại như thế nào', 'ở đây có chiếc điện thoại nào kèm tai nghe không']\n",
            "['mình cần mua một cái điện thoại mới', 'mình chưa biết bạn có thể tư vấn mình không', 'những chiếc điện thoại có giá 6 triệu trở lên sẽ được tặng kèm tai nghe']\n",
            "Len questions_cleaned: 5855\n",
            "Len answers_cleaned: 5855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad-SRi6Quu2l"
      },
      "source": [
        "remove rare word\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn6f2sLluxDd"
      },
      "source": [
        "# Create a word count dictionary for question and answer vocabulary to find the rarely occuring words\n",
        "count_mapping_dict = dict()\n",
        "def data_imporvement(text):\n",
        "    for word in text.split():\n",
        "        if word not in count_mapping_dict:\n",
        "            count_mapping_dict[word] = 1\n",
        "        else:\n",
        "            count_mapping_dict[word] += 1"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGithlVSu1J4"
      },
      "source": [
        "# find the question vocab counts\n",
        "for text in questions_cleaned:\n",
        "    data_imporvement(text)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILUwKSspuWpd"
      },
      "source": [
        "# find the answers vocab counts\n",
        "for text in answers_cleaned:\n",
        "    data_imporvement(text)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Qk8lK9vHcn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ce5f26-7db0-4ca4-c79c-feaf35cbabb1"
      },
      "source": [
        "# Creating threshold to filter out less frequent words\n",
        "# Filter out words from vocabulary whose count is less than the threshold\n",
        "# create a words to int dictionary for question words\n",
        "threshold = 40\n",
        "questionwords2int ={}\n",
        "word_number = 0\n",
        "for word, count in count_mapping_dict.items():\n",
        "         if count > threshold:\n",
        "            questionwords2int[word] =  word_number\n",
        "            word_number += 1\n",
        "\n",
        "# Verify the length of mapping dict or the number of words in the question vocabulary\n",
        "len(questionwords2int)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "372"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BEUcTnFvQgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6404ac-5504-4d40-a26f-a4465f984a9e"
      },
      "source": [
        "# Creating threshold to filter out less frequent words\n",
        "# Filter out words from vocabulary whose count is less than the threshold\n",
        "# create a words to int dictionary for answer words\n",
        "threshold = 40\n",
        "answerwords2int = {}\n",
        "word_number = 0\n",
        "for word, count in count_mapping_dict.items():\n",
        "         if count > threshold:\n",
        "            answerwords2int[word] =  word_number\n",
        "            word_number += 1\n",
        "# Verify the length of mapping dict or the number of words in the answer vocabulary\n",
        "len(answerwords2int)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "372"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9boPTWpvWiX"
      },
      "source": [
        "# <SOS> Start of string\n",
        "# <EOS> End of String\n",
        "# <PAD> for maintaining the length of input\n",
        "# <OUT> for words not used while training(filter out)\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "\n",
        "# adding token and corresponding integer mapping to existing word to integer mapping for questions\n",
        "for token in tokens:\n",
        "    questionwords2int[token] = len(questionwords2int)\n",
        "\n",
        "# adding token and corresponding integer mapping to existing word to integer mapping for answers\n",
        "for token in tokens:\n",
        "    answerwords2int[token] = len(answerwords2int)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BfeQFQCvfcd",
        "outputId": "23b36376-1aaa-488b-9042-ba1997ca525f"
      },
      "source": [
        "# create an inverse dictionary of answers2int for decoder so that after predicting the word index we can create the words\n",
        "int2answerwords = {i:w for w,i in answerwords2int.items()}\n",
        "print(int2answerwords)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'xin', 1: 'chào', 2: 'mình', 3: 'có', 4: 'thể', 5: 'giúp', 6: 'gì', 7: 'cho', 8: 'bạn', 9: 'muốn', 10: 'một', 11: 'điện', 12: 'thoại', 13: 'như', 14: 'thế', 15: 'nào', 16: 'ở', 17: 'đây', 18: 'nghe', 19: 'không', 20: 'đang', 21: 'là', 22: 'hành', 23: 'hiện', 24: 'nay', 25: 'tính', 26: 'mua', 27: 'vào', 28: 'dịp', 29: 'tết', 30: 'này', 31: 'hay', 32: 'dụng', 33: 'để', 34: 'bằng', 35: 'tiện', 36: 'ý', 37: 'chỗ', 38: 'laptop', 39: 'tốt', 40: 'được', 41: '1', 42: 'mới', 43: 'biết', 44: 'hỏi', 45: 'đi', 46: 'quần', 47: 'áo', 48: 'định', 49: 'thích', 50: 'hơn', 51: 'cần', 52: 'của', 53: 'cậu', 54: 'đâu', 55: 'vậy', 56: 'còn', 57: 'shop', 58: 'chất', 59: 'mấy', 60: 'size', 61: 'cao', 62: 'nặng', 63: 'mặc', 64: 'giá', 65: 'bao', 66: 'nhiêu', 67: 'anh', 68: 'ơi', 69: 'bán', 70: 'ạ', 71: 'chị', 72: 'm', 73: 'cũng', 74: 'á', 75: 'thử', 76: 'xem', 77: 'nữa', 78: 'tớ', 79: 'xong', 80: 'rồi', 81: 'tới', 82: 'sao', 83: 'à', 84: 'mà', 85: 'khi', 86: 'máy', 87: 'chưa', 88: 'trai', 89: 'loại', 90: 'thấy', 91: 'lắm', 92: 'mẫu', 93: 'nó', 94: 'đồng', 95: 'hồ', 96: 'ngoài', 97: 'thì', 98: 'nên', 99: 'giờ', 100: 'trình', 101: 'những', 102: 'sản', 103: 'em', 104: 'bên', 105: 'khác', 106: 'mày', 107: 'lên', 108: 'vài', 109: 'món', 110: 'ăn', 111: 'rảnh', 112: 'tao', 113: 'cái', 114: 'mọi', 115: 'người', 116: 'về', 117: 'con', 118: 'đồ', 119: 'xinh', 120: 'các', 121: 'hôm', 122: 'qua', 123: 'tiền', 124: 'hàng', 125: 'tại', 126: 'ai', 127: 'nè', 128: 'nhất', 129: 'mặt', 130: 'tôi', 131: 'nhiều', 132: 'hình', 133: 'với', 134: 'sẽ', 135: 'bảo', 136: 'lâu', 137: 'trong', 138: 'thành', 139: '3', 140: '4', 141: 'năm', 142: 'chứ', 143: 'thường', 144: 'đủ', 145: 'chắc', 146: 'trên', 147: 'ổn', 148: 'làm', 149: 'chơi', 150: 'dạo', 151: 'quá', 152: 'ra', 153: 'đã', 154: 'hết', 155: 'dạ', 156: 'phải', 157: 'từ', 158: 'nơi', 159: 'hãng', 160: 'trang', 161: 'tên', 162: 'y', 163: 'nh', 164: 'co', 165: 'c', 166: 'a', 167: 'o', 168: 'i', 169: 'thêm', 170: 'ba', 171: 'n', 172: 'nha', 173: 't', 174: 'u', 175: 'mi', 176: 'gia', 177: 'ng', 178: 'số', 179: 'hợp', 180: 'và', 181: 'nhau', 182: 'tầm', 183: 'địa', 184: 'chỉ', 185: 'xa', 186: 'nếu', 187: 'đến', 188: 'ngon', 189: '2', 190: 'học', 191: 'tập', 192: 'đẹp', 193: 'công', 194: 'nghệ', 195: 'thông', 196: 'tin', 197: 'nhỉ', 198: 'chọn', 199: 'triệu', 200: 'cầu', 201: 'yêu', 202: 'tình', 203: 'nhà', 204: 'cơ', 205: 'nhân', 206: 'viên', 207: 'lần', 208: 'đầu', 209: 'tìm', 210: 'nhé', 211: 'luôn', 212: 'chuyện', 213: 'thương', 214: 'nổi', 215: 'tiếng', 216: 'gái', 217: 'nam', 218: 'chủ', 219: 'lớn', 220: 'đó', 221: 'phí', 222: 'giới', 223: 'gần', 224: 'sài', 225: 'vì', 226: 'lại', 227: 'chi', 228: 'việc', 229: 'game', 230: 'quan', 231: 'tâm', 232: 'sách', 233: 'trường', 234: 'bộ', 235: 'môn', 236: 'ngành', 237: 'trung', 238: 'động', 239: 'ok', 240: 'sinh', 241: 'ngày', 242: '5', 243: 'nhưng', 244: 'tuần', 245: 'khoảng', 246: '20', 247: 'do', 248: 'rất', 249: 'từng', 250: 'chính', 251: 'nói', 252: 'ông', 253: 'phòng', 254: 'phố', 255: 'an', 256: 'đường', 257: 'dễ', 258: 'hai', 259: 'quận', 260: '7', 261: 'tui', 262: 'cách', 263: 'điểm', 264: 'nghĩ', 265: 'dịch', 266: 'dự', 267: 'nhỏ', 268: 'ta', 269: 'ấy', 270: 'đúng', 271: 'thời', 272: 'cùng', 273: 'tối', 274: 'theo', 275: 'cảm', 276: 'cả', 277: 'la', 278: 'rủ', 279: 'sở', 280: 'phim', 281: 'thao', 282: 'cơm', 283: 'gian', 284: 'bóng', 285: 'lúc', 286: 'đặc', 287: 'biệt', 288: 'nhạc', 289: 'du', 290: 'lịch', 291: 'đọc', 292: 'đình', 293: 'đá', 294: 'chung', 295: 'bè', 296: 'mỗi', 297: 'coi', 298: 'đức', 299: 'việt', 300: 'mẹ', 301: 'mai', 302: 'rãnh', 303: 'đấy', 304: 'cô', 305: 'trọ', 306: 'khoa', 307: 'thân', 308: 'thứ', 309: 'kinh', 310: 'tôn', 311: 'thắng', 312: 'bình', 313: 'chuyên', 314: 'tham', 315: 'đại', 316: 'tháng', 317: 'vui', 318: 'cuối', 319: 'nghiệp', 320: 'khó', 321: 'tự', 322: 'uống', 323: 'quen', 324: 'robot', 325: 'quê', 326: 'sự', 327: 'xe', 328: 'bị', 329: 'bố', 330: 'đà', 331: 'lạt', 332: 'ít', 333: 'tất', 334: 'trước', 335: 'nước', 336: 'quốc', 337: 'sống', 338: 'họ', 339: 'tuổi', 340: 'quán', 341: 'nghề', 342: 'ngủ', 343: 'sau', 344: 'lớp', 345: 'gòn', 346: 'minh', 347: 'lan', 348: 'ty', 349: 'lương', 350: 'hoặc', 351: 'gặp', 352: 'hơi', 353: 'hệ', 354: 'vẫn', 355: 'tụi', 356: 'crush', 357: 'thôi', 358: 'lý', 359: 'chúng', 360: 'đất', 361: 'nghỉ', 362: 'nhớ', 363: 'lễ', 364: 'chí', 365: 'nội', 366: 'khá', 367: 'nhiên', 368: 'nguyễn', 369: '6', 370: 'thuê', 371: 'tùy', 372: '<PAD>', 373: '<EOS>', 374: '<OUT>', 375: '<SOS>'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9r_dGPDy4__"
      },
      "source": [
        "addd tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thd4PCC2wuB7",
        "outputId": "39223d4e-d4e1-40fa-c510-caf6d46c731e"
      },
      "source": [
        "for i in range(len(answers_cleaned)):\n",
        "    answers_cleaned[i] = '<SOS> '+ answers_cleaned[i] + ' <EOS>'\n",
        "print(answers_cleaned[:3])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<SOS> mình cần mua một cái điện thoại mới <EOS>', '<SOS> mình chưa biết bạn có thể tư vấn mình không <EOS>', '<SOS> những chiếc điện thoại có giá 6 triệu trở lên sẽ được tặng kèm tai nghe <EOS>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQcfQFbXvoCX"
      },
      "source": [
        "translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH2ZvBfYw5-N"
      },
      "source": [
        "# Converting question setences to integer encoding\n",
        "question_to_int = []\n",
        "for question in questions_cleaned:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in questionwords2int:\n",
        "            ints.append(questionwords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionwords2int[word])\n",
        "    question_to_int.append(ints)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27a_v5BFw7ml"
      },
      "source": [
        "# Converting answer setences to integer encoding\n",
        "answer_to_int = []\n",
        "for answer in answers_cleaned:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerwords2int:\n",
        "            ints.append(answerwords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerwords2int[word])\n",
        "    answer_to_int.append(ints)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu6KeQF6w-Ru",
        "outputId": "83c6822c-523f-4476-e126-effa63d64c50"
      },
      "source": [
        "# Calculate the length of longest question to be used for encode model input\n",
        "question_sequence_length = max([len(question) for question in question_to_int])\n",
        "question_sequence_length"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v2W5UF_xCGR",
        "outputId": "1944701a-a43b-41ba-a285-9625a9a714f6"
      },
      "source": [
        "# Calculate the length of longest answer to be used for decoder model input\n",
        "answer_sequence_length = max([len(answer) for answer in answer_to_int])\n",
        "answer_sequence_length"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjlPBXOSxEqx",
        "outputId": "ace688b6-8a8b-4163-d413-5e9ecc8df55f"
      },
      "source": [
        "# Create the LSTM based Encoder-Decoder model using Keras Functional API\n",
        "# dimension of embedding layer\n",
        "EMBED_HID_DIM = 100\n",
        "\n",
        "# dimension of LSTM unit\n",
        "latent_dim = 100\n",
        "\n",
        "# Size of questions and answers vocab\n",
        "vocab_questions = len(questionwords2int)\n",
        "vocab_answers = len(answerwords2int)\n",
        "\n",
        "# Encoder Model creationS\n",
        "# Define an input shape.\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# Define the embedding layer\n",
        "inputs_embed = Embedding(input_dim=vocab_questions, output_dim=EMBED_HID_DIM, input_length=question_sequence_length)\n",
        "encoder_embed = inputs_embed(encoder_inputs)\n",
        "\n",
        "# DEFINE THE LSTM layer\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_embed)\n",
        "\n",
        "# create the encoder model\n",
        "model = Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "\n",
        "#decoder input shape\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# define decoder embedding layer\n",
        "decode_inputs_embed = Embedding(input_dim=vocab_answers, output_dim=EMBED_HID_DIM, input_length=answer_sequence_length)\n",
        "decoder_embed = decode_inputs_embed(decoder_inputs)\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embed,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "# Final classifier layer Dense and softmax activated\n",
        "decoder_dense = Dense(vocab_answers, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_10 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, None, 100)    37600       input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, None, 100)    37600       input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 100), (None, 80400       embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, None, 100),  80400       embedding_5[0][0]                \n",
            "                                                                 lstm_4[0][1]                     \n",
            "                                                                 lstm_4[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 376)    37976       lstm_5[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 273,976\n",
            "Trainable params: 273,976\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi9B1SIqxLSz"
      },
      "source": [
        "# Compile the Keras model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='acc')"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOMrdw0sxQQ3"
      },
      "source": [
        "data preparetion\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii92fafnxSp3"
      },
      "source": [
        "# Padding of Integer coded question and answers with <PAD> sequence\n",
        "def padding(encoder_sequences, decoder_sequences):\n",
        "    \n",
        "    encoder_input_data = pad_sequences(encoder_sequences, maxlen=question_sequence_length, dtype='int32', padding='post', truncating='post', value= questionwords2int['<PAD>'])\n",
        "    decoder_input_data = pad_sequences(decoder_sequences, maxlen=answer_sequence_length, dtype='int32', padding='post', truncating='post', value= answerwords2int['<PAD>'])\n",
        "  \n",
        "    return encoder_input_data, decoder_input_data\n",
        "\n",
        "encoder_input_data, decoder_input_data = padding(question_to_int, answer_to_int)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QBeNe2_xYv3",
        "outputId": "06148e69-1fbf-4a00-aad3-7482ee0e52b8"
      },
      "source": [
        "# Verify encoder input shape\n",
        "encoder_input_data.shape"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5855, 93)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ytEy6WLxbHR",
        "outputId": "56535aee-9189-4705-fd6a-facfecf0dea1"
      },
      "source": [
        "# Verify encoder input shape\n",
        "decoder_input_data.shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5855, 158)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ky6umbpxetx"
      },
      "source": [
        "# Function to generate batch of training data based on batch size becuase dataset is too big to fit in memory\n",
        "# encoder_input_data = as is it\n",
        "# decoder_input data = as it is\n",
        "# decoder_target_data = offset by one timestep\n",
        "\n",
        "max_source_length = question_sequence_length\n",
        "max_target_length = answer_sequence_length\n",
        "num_decoder_tokens = vocab_answers\n",
        "\n",
        "def generate_batch(X , y , batch_size = 128):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "\n",
        "            encoder_input_data = np.zeros((batch_size, max_source_length),dtype='int32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_target_length),dtype='int32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_target_length, num_decoder_tokens),dtype='int32')\n",
        "\n",
        "            for i, (input_seq, target_seq) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_seq):\n",
        "                    encoder_input_data[i, t] = word\n",
        "                for t, word in enumerate(target_seq):\n",
        "                    if t<len(target_seq)-1:\n",
        "                        decoder_input_data[i, t] = word # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        # Offset by one timestep\n",
        "                        #print(word)\n",
        "                        decoder_target_data[i, t - 1, word] = 1.\n",
        "\n",
        "\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy9YfUISxikE"
      },
      "source": [
        "# Model training configuration\n",
        "train_samples = len(question_to_int) # Total Training samples\n",
        "batch_size = 128\n",
        "epochs = 1000"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3jT0nBIxli8"
      },
      "source": [
        "X_train = encoder_input_data\n",
        "y_train = decoder_input_data"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddj-LLm_xoRc",
        "outputId": "ebfb71b8-e4fb-4523-e1a3-4e0d1fd54977"
      },
      "source": [
        "# Model fit\n",
        "#from tensorflow.keras.models import Model\n",
        "if os.path.exists(\"model/NLP_chatbot_1500.h5\"):\n",
        "  model = tf.keras.models.load_model(\"model/NLP_chatbot_1500.h5\")\n",
        "  print(\"Loaded Model\")\n",
        "  model.summary()\n",
        "else:\n",
        "  print(\"Couldn't find pretrain model, train new model.\")\n",
        "  model.fit(generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size, epochs=epochs)\n",
        "  # Save the model to disk\n",
        "  model.save('model/LSTM_chatbot.h5')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Model\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 100)    37600       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 100)    37600       input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 100), (None, 80400       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 100),  80400       embedding_3[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 376)    37976       lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 273,976\n",
            "Trainable params: 273,976\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUZZlO6Ez5qY"
      },
      "source": [
        "# Encode the input sequence to get the \"Context vectors\"\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder setup\n",
        "\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = decode_inputs_embed(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_input)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_state_input,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISKmv0Rrz-zo"
      },
      "source": [
        "# Function to decode the sequence from a decoder given the input sequence\n",
        "# input_seq = input()\n",
        "def decode_sequence(input_seq):\n",
        "        # Encode the input as state vectors.\n",
        "        states_value = encoder_model.predict(input_seq)\n",
        "        \n",
        "        # Generate empty target sequence of length 1.\n",
        "        target_seq = np.zeros((1,1))\n",
        "        \n",
        "        # Populate the first character of \n",
        "        #target sequence with the start character.\n",
        "        target_seq[0, 0] = answerwords2int['<SOS>']\n",
        "        \n",
        "        # Sampling loop for a batch of sequences\n",
        "        # (to simplify, here we assume a batch of size 1).\n",
        "        stop_condition = False\n",
        "        decoded_sentence = ' '\n",
        "        \n",
        "        while not stop_condition:\n",
        "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "            \n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_word =int2answerwords[sampled_token_index]\n",
        "            decoded_sentence += ' '+ sampled_word\n",
        "            \n",
        "            # Exit condition: either hit max length\n",
        "            # or find stop character.\n",
        "            if (sampled_word == '<EOS>' or len(decoded_sentence.split()) > 25):\n",
        "                stop_condition = True\n",
        "        \n",
        "            # Update the target sequence (of length 1).\n",
        "            target_seq = np.zeros((1,1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "            \n",
        "            # Update states\n",
        "            states_value = [h, c]\n",
        "        return decoded_sentence\n",
        "# decode_sequence(input_seq)\n",
        "# print(states_value)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXVi32pe0DNj"
      },
      "source": [
        "# Create a batch generator for batch size 1\n",
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6A3AOlC0xCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7715bad9-1c5f-4c41-9ff2-7edd03adbdf4"
      },
      "source": [
        "while (True):\n",
        "  input_seq = input(\"You: \").lower().strip()\n",
        "  if (input_seq == \"exit\"):\n",
        "    break\n",
        "\n",
        "  for i in range(0, len(questions_cleaned)-1) :\n",
        "    if(input_seq == questions_cleaned[i]):\n",
        "      # decoded_sentence = decode_sequence(input_seq)\n",
        "      print(\"Ans: \", answers_cleaned[i:i+1][0][5:-5])\n",
        "      break\n",
        "  if(input_seq not in questions_cleaned):\n",
        "    (input_seq, actual_output), target_output = next(train_gen)\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"Ans: \", decoded_sentence[:-5].strip())\n",
        "      \n",
        "# len(questions_cleaned)     "
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You: xin chào bạn\n",
            "Ans:  nhỏ nhỏ nhỏ tham chắc lâu chắc chắc lâu chắc nay nay đại xem dễ xem cậu cậu giúp giúp con theo cô theo giú\n",
            "You: bạn tên gì\n",
            "Ans:   mình tên phạm thị lan anh \n",
            "You: bạn là ai\n",
            "Ans:   xin chào mình là robot tiếp tân tại trường đh tôn đức thắng \n",
            "You: thích đi chơi hay đi học\n",
            "Ans:  nhỏ nhỏ nhỏ tham chắc lâu chắc chắc lâu chắc nay nay đại xem dễ xem cậu cậu giúp giúp con theo cô theo giú\n",
            "You: exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVjPgNVv0GpE"
      },
      "source": [
        "# Predict the target sentence and compare with the actual target sentence given a source sentence\n",
        "#k+=1\n",
        "#(input_seq, actual_output), target_output = next(train_gen)\n",
        "#decoded_sentence = decode_sequence(input_seq)\n",
        "#print('Input Source sentence:',questions_cleaned[k:k+1][0])\n",
        "#print('Actual Target Sentence:', answers_cleaned[k:k+1][0][5:-5])\n",
        "#print('Predicted Target Sentence:', decoded_sentence)"
      ],
      "execution_count": 97,
      "outputs": []
    }
  ]
}